import os
import requests
import traceback
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Response
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import Dict, Any, List, Optional
import io
import base64
from contextlib import redirect_stdout, redirect_stderr
import tempfile
import shutil
import subprocess

# Third-party libraries
import numpy as np
import sympy as sp
import scipy
import matplotlib.pyplot as plt
import google.generativeai as genai

# --- FastAPI and CORS Setup ---
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:8080", "http://127.0.0.1:8080"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Environment and API Key Loading ---
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
TEXLIVE_PATH = r"D:\texlive\2025\bin\windows\pdflatex.exe"

# Configure the Google Generative AI client
if GEMINI_API_KEY:
    try:
        genai.configure(api_key=GEMINI_API_KEY)
    except Exception as e:
        print(f"Warning: Failed to configure Google GenAI client: {e}")


# --- Pydantic Models ---
class AIRequest(BaseModel):
    provider: str
    model: str
    task: str
    data: Dict[str, Any]

class ExecuteRequest(BaseModel):
    code: str

class ConnectionTestRequest(BaseModel):
    provider: str
    
class LatexReportRequest(BaseModel):
    problem: str
    solution: str
    status: str # "success" or "failed"
    error_reports: Optional[List[str]] = None
    provider: str
    model: str

class CodeExecutionResult(BaseModel):
    success: bool
    output: str
    error: str
    image: str | None = None


# --- Prompt Loading ---
def load_prompt(filename: str, data: Dict[str, Any]) -> str:
    try:
        with open(os.path.join("prompts", filename), "r", encoding="utf-8") as f:
            prompt_template = f.read()
        return prompt_template.format(**data)
    except FileNotFoundError:
        raise HTTPException(status_code=500, detail=f"Prompt file not found: {filename}")
    except KeyError as e:
        raise HTTPException(status_code=400, detail=f"Missing data for prompt placeholder: {e}")


# --- AI Call Handlers ---
async def call_ai_provider(provider: str, model: str, prompt: str) -> str:
    if provider == "google":
        return await call_gemini_api(model, prompt)
    elif provider == "openai":
        return await call_openai_api(model, prompt)
    elif provider == "deepseek":
        return await call_deepseek_api(model, prompt)
    else:
        raise HTTPException(status_code=400, detail="Invalid AI provider.")

async def call_gemini_api(model: str, prompt: str) -> str:
    try:
        model_instance = genai.GenerativeModel(model)
        response = await model_instance.generate_content_async(prompt)
        if response.parts:
            return "".join(part.text for part in response.parts)
        else:
            return "Error: The AI model did not provide a valid response."
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Google GenAI Error: {str(e)}\n{traceback.format_exc()}")

async def call_openai_api(model: str, prompt: str) -> str:
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {OPENAI_API_KEY}"}
    json_payload = {"model": model, "messages": [{"role": "user", "content": prompt}]}
    try:
        response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=json_payload, timeout=120)
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    except requests.exceptions.RequestException as e:
        raise HTTPException(status_code=500, detail=f"OpenAI API Error: {str(e)}\n{traceback.format_exc()}")

async def call_deepseek_api(model: str, prompt: str) -> str:
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {DEEPSEEK_API_KEY}"}
    json_payload = {"model": model, "messages": [{"role": "user", "content": prompt}]}
    try:
        response = requests.post("https://api.deepseek.com/v1/chat/completions", headers=headers, json=json_payload, timeout=120)
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    except requests.exceptions.RequestException as e:
        raise HTTPException(status_code=500, detail=f"DeepSeek API Error: {str(e)}\n{traceback.format_exc()}")


# --- API Endpoints ---
@app.post("/api/test-connection")
async def test_connection(request: ConnectionTestRequest):
    # ... (omitted for brevity, no changes)

@app.post("/api/call-ai")
async def call_ai_endpoint(request: AIRequest):
    # ... (omitted for brevity, no changes)

@app.post("/api/execute-python", response_model=CodeExecutionResult)
async def execute_python(request: ExecuteRequest):
    # ... (omitted for brevity, no changes)

@app.post("/api/generate-latex-pdf")
async def generate_latex_pdf(request: LatexReportRequest):
    max_retries = 5
    latex_code = ""
    last_error_log = ""

    template_file = "report_template.tex" if request.status == "success" else "report_template_failed.tex"
    
    try:
        with open(os.path.join("prompts", template_file), "r", encoding="utf-8") as f:
            latex_template = f.read()
    except FileNotFoundError:
        raise HTTPException(status_code=500, detail=f"LaTeX template '{template_file}' not found.")

    # Initial code generation
    latex_code = latex_template.replace("%%% PROBLEM_STATEMENT %%%", request.problem)
    latex_code = latex_code.replace("%%% SOLUTION_CONTENT %%%", request.solution)
    if request.status == "failed" and request.error_reports:
        formatted_errors = "\n\n".join([f"--- Error Report {i+1} ---\n{report}" for i, report in enumerate(request.error_reports)])
        latex_code = latex_code.replace("%%% ERROR_REPORTS %%%", formatted_errors)

    for attempt in range(max_retries):
        temp_dir = tempfile.mkdtemp()
        tex_path = os.path.join(temp_dir, "report.tex")
        pdf_path = os.path.join(temp_dir, "report.pdf")
        log_path = os.path.join(temp_dir, "report.log")

        try:
            with open(tex_path, "w", encoding="utf-8") as f:
                f.write(latex_code)

            process = subprocess.run(
                [TEXLIVE_PATH, "-interaction=nonstopmode", "-output-directory", temp_dir, tex_path],
                capture_output=True, text=True, timeout=30, encoding='utf-8', errors='ignore'
            )

            if process.returncode == 0 and os.path.exists(pdf_path):
                return FileResponse(pdf_path, media_type='application/pdf', filename="Mathmatica_Report.pdf")

            with open(log_path, "r", encoding="utf-8", errors='ignore') as log_file:
                last_error_log = log_file.read()
            
            fix_prompt = load_prompt("fix_latex_code_prompt.txt", {"error_log": last_error_log, "latex_code": latex_code})
            fixed_code = await call_ai_provider(request.provider, request.model, fix_prompt)
            
            if "```latex" in fixed_code:
                fixed_code = fixed_code.split("```latex")[1].split("```")[0].strip()
            
            latex_code = fixed_code

        except subprocess.TimeoutExpired:
            last_error_log = "LaTeX compilation timed out after 30 seconds."
        except Exception as e:
            last_error_log = f"An unexpected error occurred: {str(e)}\n{traceback.format_exc()}"
            break
        finally:
            shutil.rmtree(temp_dir)

    raise HTTPException(status_code=500, detail=f"Failed to generate PDF after {max_retries} attempts. Last error log:\n{last_error_log}")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
